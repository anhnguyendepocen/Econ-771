---
title: "Module 2 Empirical Exercise"
author: "Econ 771: Health Economics II"
date: "Due: Friday, October 9"
output: 
  bookdown::html_document2:
    toc: TRUE
    toc_float: TRUE
    theme: darkly
    number_sections: FALSE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Overview
In this assignment, we're going to work through some applied issues related to instrumental variables. For a long time, IV (or 2SLS) was the primary identification strategy for applied empirical micro, but it fell out of favor as people became more aware of the assumptions underlying the estimator and better understand what IV actually estimates (not the ATE in most cases). People also started to find other strategies that were more compelling in some applications (and of course with some other assumptions).

To begin, please "accept" the assignment on our GitHub classroom page. This will initialize a GitHub repository where you can edit and ultimately craft your answers in your data management/econometrics software of choice.  

You will "turn in" your repository to me with your answers and all supporting documents. Please create a final document with your main answers and analyses in a PDF. Please also be sure to include in your repository all of your supporting code files. Practice writing good code and showing me only what I would need to recreate your results.


# Resources and data
The data for this assignment comes from two sources:

1. **Medicare Advantage Data**: The Centers for Medicare and Medicaid services provides a ton of Medicare Advantage data. My [Medicare Advantage GitHub Repo](https://github.com/imccart/Medicare-Advantage) should help for those interested in building a comprehensive MA dataset. For this exercise, we'll just use the data on MA penetration and benchmark rates. The raw data are available on our AWS AMI in the "Medicare Advantage" folder, and my GitHub repo has designated code files for each of these two data sources. To make things simpler, you can just use the December penetration data from each year rather than reading in all months of data and averaging. But you're welcome to do it either way.

2. **Area Health Resources Files**: We'll use the [AHRF](https://data.hrsa.gov/topics/health-workforce/ahrf) to form county-level measures of healthcare expenditures. We could also use the HCRIS from exercise 1 to create something on our own, but we'll stick with the AHRF so that we get exposure to another common dataset. These data are available for direct download in the link above. I've also placed all of the data in an "AHRF" folder on our AWS AMI. Please use the "standardized, risk-adjusted per capita Medicare cost" as our county-level measure of expenditure. *hint: you should look into the `SAScii` package to help read the ascii data into `R` using the `SAS` input file.*


# Questions
In your GitHub repository, please be sure to clearly address/answer the following questions.

1. Provide and discuss a table of simple summary statistics showing the mean, standard deviation, min, and max of the Medicare Advantage penetration rate, benchmark payments, and utilization measures. 

2. Plot the mean county-level utilization against the mean MA penetration rate for each year (just one scatter plot with the mean of each variable by year). 

3. Using the MA benchmark as an IV for MA penetration rates, estimate the effects of MA penetration on utilization. This is analogous to the specification in Baicker, Chernew, and Robbins (2013).

4. Present the "first stage" and "reduced form" results of your IV estimates in Part 4.

5. Present selected specification/robustness checks of your IV and discuss two weaknesses of this identification strategy.


# Solutions
Here are my answers. There are lots of different ways to answer these questions. For example, some of the main variables may show up in more than one of the datasets, and you may have chosen a different source. There are also lots of different ways to filter the data. As long as you are transparent (i.e., I can see what you did in the code), then this isn't a problem. The point of this is to get some practice doing standard applied work with **real** data. 

1. Final [solutions](../solutions/exercise2/solutions.pdf) in PDF. My actual `R Markdown` document is [here](../solutions/exercise2/solutions.Rmd), and [here](../solutions/exercise2/solutions.tex) is the `tex` file in case anyone is interested.
2. Analysis file [here](../solutions/exercise2/analysis.R). This is where I do the actual analysis. Notice that in this file, I remove a bunch of large objects and save the workspace. I then call the workspace in my markdown file.
3. Code to read in the MA benchmark data is [here](../solutions/exercise2/1_benchmark.R).
4. Code to read in the MA penetration data [here](../solutions/exercise2/2_penetration.R).
4. Code to read in the AHRF data [here](../solutions/exercise2/3_ahrf.R).

